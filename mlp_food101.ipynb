{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP mixer proposes a way to use just mlps for vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its not better but it is competetive (at large scale) and could be researched upon ->  due to speed of infernece\n",
    "plus has better tolerence to pixel shuffling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses channel mixing and token mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms,datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard MLP block == two linear layers and a GELU nonlinearity.\n",
    "    The first layer expands the dimension to mlp_dim, then shrinks back.\n",
    "    fc-glu-fc\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, mlp_dim):\n",
    "        super(MlpBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, in_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Mixer block that separately mixes tokens and channels.\n",
    "    It first applies token mixing (across patches) and then channel mixing (within features).\n",
    "    part 1 then 2 in the arch diagram\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tokens, hidden_dim, tokens_mlp_dim, channels_mlp_dim,drop_path=0.1):\n",
    "        super(MixerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        # (1) MLP applied to the token dimension (each channel separately)\n",
    "        self.token_mixing = MlpBlock(num_tokens, tokens_mlp_dim)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        # (2) MLP applied to the channel dimension (each token separately)\n",
    "        self.channel_mixing = MlpBlock(hidden_dim, channels_mlp_dim)\n",
    "        self.drop_path = drop_path\n",
    "\n",
    "                \n",
    "    # def forward(self, x):\n",
    "    #     # x: (batch, num_tokens, hidden_dim)\n",
    "\n",
    "    #     \"\"\"\n",
    "    #     # Token mixing\n",
    "    #     \"\"\"\n",
    "    #     y = self.norm1(x)\n",
    "    #     y = y.transpose(1, 2)  # (B, hidden_dim, num_tokens)\n",
    "    #     y = self.token_mixing(y)\n",
    "    #     y = y.transpose(1, 2)  # back to (B, num_tokens, hidden_dim)\n",
    "    #     x = x + y  # skip connection\n",
    "    #     \"\"\"\n",
    "    #     # Channel mixing\n",
    "    #     \"\"\"\n",
    "    #     y = self.norm2(x)\n",
    "    #     y = self.channel_mixing(y)\n",
    "    #     return x + y  # skip connection\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Token mixing\n",
    "        if self.training and torch.rand(1).item() < self.drop_path:\n",
    "            y = 0\n",
    "        else:\n",
    "            y = self.norm1(x)\n",
    "            y = y.transpose(1, 2)\n",
    "            y = self.token_mixing(y)\n",
    "            y = y.transpose(1, 2)\n",
    "            if self.drop_path > 0:\n",
    "                y = y / (1 - self.drop_path)\n",
    "        x = x + y\n",
    "        \n",
    "        # Channel mixing\n",
    "        if self.training and torch.rand(1).item() < self.drop_path:\n",
    "            y = 0\n",
    "        else:\n",
    "            y = self.norm2(x)\n",
    "            y = self.channel_mixing(y)\n",
    "            if self.drop_path > 0:\n",
    "                y = y / (1 - self.drop_path)\n",
    "        return x + y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpMixer(nn.Module):\n",
    "    \"\"\"\n",
    "    The full MLP-Mixer network.\n",
    "    Given an input image, it splits it into patches via a convolution (\"stem\"),\n",
    "    then processes the resulting tokens with several Mixer blocks,\n",
    "\n",
    "    applies a final layer norm-> global average pooling-> linear classifier.\n",
    "    \n",
    "    Rn using CIFAR-100 (32×32 images), we set a small patch size (4×4) as imagent is too big\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, num_blocks, patch_size, hidden_dim,\n",
    "                 tokens_mlp_dim, channels_mlp_dim, image_size=32, in_channels=3):\n",
    "        super(MlpMixer, self).__init__()\n",
    "\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        \"\"\"\n",
    "        # (1) The stem Conv2d splits the image into non-overlapping patches. \n",
    "        #     START OF THE PAPER WE SPLIT THE IMAGE INTO NON OVERLAPPING PATCH\n",
    "        #     CNN does the same sort of thing\n",
    "        \"\"\"\n",
    "        self.stem = nn.Conv2d(in_channels, hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_tokens = (image_size // patch_size) ** 2\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        # (2) makes them into list of mixer block\n",
    "            \n",
    "        \"\"\"\n",
    "        # Create a list of Mixer blocks.\n",
    "        self.mixer_blocks = nn.ModuleList([\n",
    "            MixerBlock(num_tokens=self.num_tokens, hidden_dim=hidden_dim,\n",
    "                       tokens_mlp_dim=tokens_mlp_dim, channels_mlp_dim=channels_mlp_dim)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        # Final layer normalization before classifiing\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # zero initialize the weights as in paper given\n",
    "        self.head = nn.Linear(hidden_dim, num_classes)\n",
    "        nn.init.zeros_(self.head.weight)\n",
    "        if self.head.bias is not None:\n",
    "            nn.init.zeros_(self.head.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, in_channels, image_size, image_size)\n",
    "        x = self.stem(x)  # → (B, hidden_dim, H', W') where H' = image_size/patch_size\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)  # → (B, num_tokens, hidden_dim)\n",
    "        for block in self.mixer_blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)  # global average pooling\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    # Data augmentation and normalization for training;\n",
    "    # simple normalization for now , ask proff for better ones for smaller dataset/ larger\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \"\"\"\n",
    "    Images are normalized using mean and standard deviation\n",
    "    Data Augmentation: Random Horizontal Flip\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Load the Food-101 dataset\n",
    "    train_dataset = datasets.Food101(root='./data', split='train', transform=transform, download=True)\n",
    "    test_dataset = datasets.Food101(root='./data', split='test', transform=transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on FOOD-101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 4.2919, Test Accuracy: 7.99%\n",
      "Epoch [2/50], Loss: 4.0319, Test Accuracy: 11.97%\n",
      "Epoch [3/50], Loss: 3.8506, Test Accuracy: 14.63%\n",
      "Epoch [4/50], Loss: 3.7238, Test Accuracy: 16.52%\n",
      "Epoch [5/50], Loss: 3.6250, Test Accuracy: 18.15%\n",
      "Epoch [6/50], Loss: 3.5340, Test Accuracy: 20.48%\n",
      "Epoch [7/50], Loss: 3.4350, Test Accuracy: 21.96%\n",
      "Epoch [8/50], Loss: 3.3497, Test Accuracy: 23.68%\n",
      "Epoch [9/50], Loss: 3.2571, Test Accuracy: 25.14%\n",
      "Epoch [10/50], Loss: 3.1602, Test Accuracy: 28.87%\n",
      "Epoch [11/50], Loss: 3.0509, Test Accuracy: 31.07%\n",
      "Epoch [12/50], Loss: 2.9512, Test Accuracy: 33.67%\n",
      "Epoch [13/50], Loss: 2.8386, Test Accuracy: 36.35%\n",
      "Epoch [14/50], Loss: 2.7121, Test Accuracy: 39.20%\n",
      "Epoch [15/50], Loss: 2.5822, Test Accuracy: 41.70%\n",
      "Epoch [16/50], Loss: 2.4541, Test Accuracy: 46.69%\n",
      "Epoch [17/50], Loss: 2.3205, Test Accuracy: 50.39%\n",
      "Epoch [18/50], Loss: 2.1790, Test Accuracy: 55.00%\n",
      "Epoch [19/50], Loss: 2.0324, Test Accuracy: 57.97%\n",
      "Epoch [20/50], Loss: 1.8838, Test Accuracy: 61.92%\n",
      "Epoch [21/50], Loss: 1.7650, Test Accuracy: 65.83%\n",
      "Epoch [22/50], Loss: 1.6487, Test Accuracy: 69.26%\n",
      "Epoch [23/50], Loss: 1.5201, Test Accuracy: 72.61%\n",
      "Epoch [24/50], Loss: 1.4390, Test Accuracy: 74.57%\n",
      "Epoch [25/50], Loss: 1.3555, Test Accuracy: 76.98%\n",
      "Epoch [26/50], Loss: 1.2512, Test Accuracy: 79.32%\n",
      "Epoch [27/50], Loss: 1.1979, Test Accuracy: 80.64%\n",
      "Epoch [28/50], Loss: 1.1974, Test Accuracy: 81.51%\n",
      "Epoch [29/50], Loss: 1.1780, Test Accuracy: 81.83%\n",
      "Epoch [30/50], Loss: 1.1466, Test Accuracy: 81.98%\n",
      "Epoch [31/50], Loss: 1.1267, Test Accuracy: 81.98%\n",
      "Epoch [32/50], Loss: 1.1533, Test Accuracy: 82.08%\n",
      "Epoch [33/50], Loss: 1.1394, Test Accuracy: 82.23%\n",
      "Epoch [34/50], Loss: 1.1540, Test Accuracy: 82.45%\n",
      "Epoch [35/50], Loss: 1.1710, Test Accuracy: 83.08%\n",
      "Epoch [36/50], Loss: 1.1720, Test Accuracy: 83.07%\n",
      "Epoch [37/50], Loss: 1.1662, Test Accuracy: 83.74%\n",
      "Epoch [38/50], Loss: 1.1563, Test Accuracy: 84.11%\n",
      "Epoch [39/50], Loss: 1.1629, Test Accuracy: 84.55%\n",
      "Epoch [40/50], Loss: 1.1945, Test Accuracy: 84.72%\n",
      "Epoch [41/50], Loss: 1.1655, Test Accuracy: 85.13%\n",
      "Epoch [42/50], Loss: 1.1844, Test Accuracy: 82.32%\n",
      "Epoch [43/50], Loss: 1.1900, Test Accuracy: 84.17%\n",
      "Epoch [44/50], Loss: 1.2084, Test Accuracy: 82.21%\n",
      "Epoch [45/50], Loss: 1.2308, Test Accuracy: 82.25%\n",
      "Epoch [46/50], Loss: 1.2216, Test Accuracy: 80.62%\n",
      "Epoch [47/50], Loss: 1.2468, Test Accuracy: 81.19%\n",
      "Epoch [48/50], Loss: 1.2523, Test Accuracy: 82.46%\n",
      "Epoch [49/50], Loss: 1.2719, Test Accuracy: 82.60%\n",
      "Epoch [50/50], Loss: 1.2946, Test Accuracy: 81.92%\n",
      "Training complete :)\n"
     ]
    }
   ],
   "source": [
    "# Set up\n",
    "    # We choose a small hidden_dim  but still get overfitting\n",
    "# model = MlpMixer(\n",
    "#         num_classes=100,     # 100 classes\n",
    "#         num_blocks=8,\n",
    "#         patch_size=4,\n",
    "#         hidden_dim=256,      # Hidden channel size\n",
    "#         tokens_mlp_dim=128,   # Hidden dimension for token-mixing MLP\n",
    "#         channels_mlp_dim=512, # Hidden dimension for channel-mixing MLP\n",
    "#         image_size=32,       # CIFAR 32x32\n",
    "#         in_channels=3        # (RGB)\n",
    "#     )\n",
    "# model.to(device)\n",
    "\n",
    "model = MlpMixer(\n",
    "    num_classes=101,             # Food-101 has 101 classes\n",
    "    num_blocks=8,               # More Mixer blocks for larger images\n",
    "    patch_size=8,               # 16x16 patch for 224x224 input\n",
    "    hidden_dim=128,              # Hidden dimension size\n",
    "    tokens_mlp_dim=256,         # Token mixing MLP size\n",
    "    channels_mlp_dim=512,       # Channel mixing MLP size\n",
    "    image_size=224,              # Updated input size\n",
    "    in_channels=3\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.3e-2, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "epochs=50\n",
    "\n",
    "print(\"Starting training on FOOD-101\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        train_acc = evaluate(model, train_loader, device)\n",
    "        scheduler.step()\n",
    "        avg_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Train Accuracy: {train_acc * 100:.2f}%\")\n",
    "\n",
    "print(\"Training complete :)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, acc\n\u001b[0;32m---> 22\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(\u001b[43mmodel\u001b[49m, test_loader, criterion)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# ✅ Evaluation Loop\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    return avg_loss, acc\n",
    "val_loss, val_acc = evaluate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things we think we should fix :\n",
    "\n",
    "Have better regularization as being overfit\n",
    "\n",
    "maybe change in LR\n",
    "\n",
    "have larger data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout\n",
    "\n",
    "Stochastic Depth ??\n",
    "\n",
    "RandAugment ??\n",
    "\n",
    "Mixup\n",
    "\n",
    "Weight Decay (L2 regularization)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
